"""
Env setting and generation.
"""
import math
from functools import reduce
import numpy as np
import pandas as pd

# ----------------------------------------------------------------------------
# Env parameter settings.
# NOTE: Some of them are discarded when using real job trace.
# ----------------------------------------------------------------------------

# HINT: `num_tasks` should be large to minimize the affect of intra-job co-execution
MIN_TASK_NUM_IN_TG = 50 #æ¯ä¸ªä»»åŠ¡ç»„æœ€å°‘ä»»åŠ¡æ•°
MAX_TASK_NUM_IN_TG = 50 #æ¯ä¸ªä»»åŠ¡ç»„æœ€å¤šä»»åŠ¡æ•°

MIN_TG_NUM = 3 #æœ€å°‘ä»»åŠ¡ç»„æ•°é‡
MAX_TG_NUM = 3 #æœ€å¤šä»»åŠ¡ç»„æ•°é‡

JOB_NUM = 250 #jobæ€»æ•°
SITE_NUM = 100 #æœåŠ¡å™¨æ•°é‡

MAX_ARRIVAL_TU = 250  # HINT: This should >= JOB_NUM #æœ€å¤§jobåˆ°è¾¾æ—¶é—´

# HINT: System utilization is used to tune the inter-job arrivals
UTILIZATION_FACTOR = 0.75 #ç³»ç»Ÿåˆ©ç”¨ç‡

# The parameter for Zipf distribution
ALPHA = 0 #å¥‡æ™®å¤«å‚æ•°

# Setting SCALE_INTER_ARRIVALS \to 0 to make a fierce contention
SCALE_INTER_ARRIVALS = 0.005 #ä½œä¸šåˆ°è¾¾é—´éš”ç¼©æ”¾å› å­ï¼Œæ§åˆ¶ä½œä¸šåˆ°è¾¾çš„å¯†é›†ç¨‹åº¦ï¼Œå€¼è¶Šå°ç«äº‰è¶Šæ¿€çƒˆ

MIN_AS_NUM_FOR_TG = 6 #æ¯ä¸ªä»»åŠ¡ç»„çš„æœ€å°ç«™ç‚¹æ€»æ•°
# HINT: A larger `MAX_AS_NUM_FOR_TG` leads to a better performance of SJF algorithms
MAX_AS_NUM_FOR_TG = 6  # HINT: This should <= SITE_NUM #æ¯ä¸ªä»»åŠ¡ç»„çš„æœ€å¤§ç«™ç‚¹æ€»æ•°

# Parameters for generating the computing capacities
MIN_MU = 5 #ç«™ç‚¹æœ€å°è®¡ç®—èƒ½åŠ›
# HINT: `MAX_MU` should be small to minimize the affect of intra-job co-execution.
#  However, a smaller it will lead to a larger computation overhead of order scheduling
MAX_MU = 5 #ç«™ç‚¹æœ€å¤§è®¡ç®—èƒ½åŠ›

# In heterogenous mode, the tasks of any job can have different running times
HOMOGENEOUS_MODE = "HOMOGENEOUS_MODE"
HETEROGENOUS_MODE = "HETEROGENOUS_MODE"
MODE = HETEROGENOUS_MODE
#æ¨¡å¼é—®é¢˜ï¼Œè¿™é‡Œè®¾ç½®ä¸ºå¼‚æ„

# Parameters for long-tail distribution in the heterogenous mode
RUNNING_TIME_MU = 1
RUNNING_TIME_SIGMA = 0.5
# ä»»åŠ¡è¿è¡Œæ—¶é—´æœä»é•¿å°¾åˆ†å¸ƒï¼ŒRUNNING_TIME_MU/SIGMAå®šä¹‰è¿è¡Œæ—¶é—´çš„æ­£æ€åˆ†å¸ƒå‚æ•°
# ----------------------------------------------------------------------------
# Where are the generated info. stored.
# ----------------------------------------------------------------------------
jobs = []
sites = []
NUM_TASKS = 0
# è¿™é‡Œè®¾ç½®åˆå€¼ï¼šä½œä¸š ç«™ç‚¹ ä»»åŠ¡æ€»é‡

class Job(object):
    def __init__(self, index, task_groups, arrival_time):
        self.index = index #ä½œä¸šå”¯ä¸€æ ‡è¯†
        self.task_groups = task_groups #åŒ…å«çš„ä»»åŠ¡ç»„åˆ—è¡¨
        self.arrival_time = arrival_time #ä½œä¸šåˆ°è¾¾æ—¶é—´
        self.available_sites = None  # The set S_g for this job J_g
        #ä½œä¸šå¯ç”¨ç«™ç‚¹é›†åˆ
        self.completion_time = None
        #ä½œä¸šå®Œæˆæ—¶é—´

    def set_available_sites(self):
        """
        Calculate S_g (the join of the AS sets of each task group).
        """
        self.available_sites = reduce(lambda x, y: x | y,
                                      [tg.available_sites for tg in self.task_groups])
        #è®¡ç®—ä½œä¸šå¯ç”¨ç«™ç‚¹é›†åˆ

    def reset(self):
        self.completion_time = None
    #é‡ç½®ä½œä¸šå®Œæˆæ—¶é—´ï¼Œç”¨äºæ–°ä¸€è½®è°ƒåº¦è°ƒè¯•

    def _collect_task_group_info(self):
        info = ""
        for tg in self.task_groups:
            info += "\t" + str(tg) + "\n"
        return info
    #æ”¶é›†ä½œä¸šä¸­æ‰€æœ‰ä»»åŠ¡ç»„çš„è¯¦ç»†ä¿¡æ¯ï¼Œå­—ç¬¦ä¸²è¡¨è¾¾

    def __str__(self):
        return "job index: {0}\nnum. of task groups: {1}, arrival time: {2}, S_g: {3}\n" \
               "task group info:\n{4}".format(self.index,
                                              len(self.task_groups),
                                              self.arrival_time,
                                              {site.index for site in self.available_sites},
                                              self._collect_task_group_info())
    #å®šä¹‰ä½œä¸šå¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
#ä»»åŠ¡ç»„
class TaskGroup(object):
    def __init__(self, index, job, num_tasks, available_sites):
        self.index = index  #ä»»åŠ¡ç»„å”¯ä¸€æ ‡è¯†
        self.job = job  #ä»»åŠ¡ç»„æ‰€å±ä½œä¸š
        self.num_tasks = num_tasks #åŒ…å«çš„ä»»åŠ¡æ•°é‡
        self.num_unfinished_tasks = num_tasks #æœªå®Œæˆä»»åŠ¡æ•°é‡

        self.available_sites = available_sites  # The set S_g^k for this task group T_g^k
        #å¯ç”¨ç«™ç‚¹é›†åˆ

        # Real execution time of each task in this task group.
        # In Alibaba trace, we use the instance duration as the task's running time.
        # All instances of the same task group has the same execution time
        self.real_running_time = None
        #å®é™…è¿è¡Œæ—¶é—´
    def reset(self):
        self.num_unfinished_tasks = self.num_tasks

    def __str__(self):
        return "index: {0}, num. of tasks: {1}, ASs: {2}, Each task's running time: {3}".format(
            self.index, self.num_tasks, {site.index for site in self.available_sites},
            self.real_running_time
        )


class Site(object):
    def __init__(self, index, capacities):
        self.index = index
        self.capacities = capacities  # A list of \mu_m^g for each job on this site S_m
        #é’ˆå¯¹ä¸åŒä½œä¸šçš„è®¡ç®—èƒ½åŠ›åˆ—è¡¨

        # The estimated backlog size of this site.
        # Here we use the word "estimated" because it is the number of allocated time units,
        # not the actual time duration for running tasks
        self.estimated_bklg_size = 0
        #åŸºäºåˆ†é…æ—¶é—´å•ä½çš„ç§¯å‹ä¼°è®¡ï¼Œç”¨äºè°ƒåº¦å†³ç­–
        # The actual time duration for running tasks
        self.true_bklg_size = 0
        #å®é™…è¿è¡Œæ—¶é—´çš„ç§¯å‹ï¼Œç”¨äºæ€§èƒ½è¯„ä¼°

    def __str__(self):
        return "index: {0}, capacities: {1}".format(
            self.index, [cap for cap in self.capacities]
        )

    def reset(self):
        self.estimated_bklg_size = 0


def create_env():
    """
    Synthesize a scheduling environment.
    """
    # Generate each job's non-repeat arrival time
    arrivals = np.arange(MAX_ARRIVAL_TU)
    np.random.shuffle(arrivals)
    #ç”Ÿæˆä½œä¸šåˆ°è¾¾æ—¶é—´å¹¶ä¸”éšæœºæ‰“ä¹±

    # Generate the number of task groups for each job
    num_task_groups = np.random.randint(MIN_TG_NUM, MAX_TG_NUM + 1, size=JOB_NUM)
    #ç”Ÿæˆæ¯ä¸ªä½œä¸šçš„ä»»åŠ¡ç»„æ•°é‡

    # Generate sites
    # If a site is not available to some job, the corresponding \mu should be zero. We just skip it here.
    # In real scenarios, you should set \mu based on the resource request and equipment.
    capacities = np.random.randint(MIN_MU, MAX_MU + 1, size=(SITE_NUM, JOB_NUM))
    #éšæœºç”ŸæˆSITE_NUMÃ—JOB_NUMçš„èƒ½åŠ›çŸ©é˜µ(Î¼_m^g)
    for site_idx in range(SITE_NUM):
        site = Site(site_idx, capacities=capacities[site_idx])
        sites.append(site)
    #ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºSiteå¯¹è±¡å¹¶åŠ å…¥å…¨å±€sitesåˆ—è¡¨
    

    for job_idx in range(JOB_NUM):
        job = Job(job_idx, task_groups=None, arrival_time=arrivals[job_idx])

        # Generate the number of tasks for this task group
        num_tasks = np.random.randint(MIN_TASK_NUM_IN_TG, MAX_TASK_NUM_IN_TG + 1, size=num_task_groups[job_idx])
        # Generate AS set for this task group
        num_available_sites = np.random.randint(MIN_AS_NUM_FOR_TG, MAX_AS_NUM_FOR_TG + 1, size=num_task_groups[job_idx])
        #ç”Ÿæˆä½œä¸šï¼šä»»åŠ¡ç»„æ•°é‡å’Œå¯ç”¨ç«™ç‚¹æ•°

        tgs = []
        for tg_idx in range(num_task_groups[job_idx]):
            shuffled_sites = np.arange(SITE_NUM)
            np.random.shuffle(shuffled_sites)
            as_set = shuffled_sites[:num_available_sites[tg_idx]]
            available_sites = {site for site in sites if site.index in as_set}

            tg = TaskGroup(tg_idx, job, num_tasks=num_tasks[tg_idx], available_sites=available_sites)
        # ä¸ºæ¯ä¸ªä½œä¸šç”Ÿæˆä»»åŠ¡ç»„
        # è®¾ç½®ä»»åŠ¡è¿è¡Œæ—¶é—´(å¼‚æ„/åŒæ„æ¨¡å¼)
            global MODE
            if MODE == HETEROGENOUS_MODE:
                # Generate each task's real running time with a long-tail distribution
                tg.real_running_time = int(np.ceil(np.random.lognormal(mean=RUNNING_TIME_MU,
                                                                       sigma=RUNNING_TIME_SIGMA,
                                                                       size=1)))

            elif MODE == HOMOGENEOUS_MODE:
                # Set all task's running time as 1
                tg.real_running_time = 1

            tgs.append(tg)

        job.task_groups = tgs
        job.set_available_sites()

        jobs.append(job)

    global NUM_TASKS
    #ç»Ÿè®¡ä»»åŠ¡æ€»æ•°
    for job in jobs:
        NUM_TASKS += sum(tg.num_tasks for tg in job.task_groups)


def from_trace():
    """
    Read from trace. In this data, a job contains multiple tasks, different tasks executes
    different computing logics. Instance is the smallest scheduling unit of batch workload,
    and all instances within a task execute exactly the same binary with the same resource
    request, but with different input data. Thus, `task' in this data corresponds to task
    group in our model, and `instance' corresponds to task.

    NOTE:
        1. We filter out large task groups (#tasks > 500) since these task groups will be
        outstanding significantly, which makes the comparison less obvious.
        2. Task durations and job arrivals are derived from the events record
    """
    # 1. Read from trace
    df = pd.read_table("../trace/batch_task.csv")

    data = {}  # Save the task group info
    arrivals = {}  # Save the job arrival times
    job_index = -1
    tg_index = 0

    id_list = []
    for line in df.values:
        line = line[0].split(",")

        # The three data segments are needed
        num_tasks = int(line[4])
        # Filter out large task groups
        if num_tasks > 500:
            continue
        job_id = int(line[2])
        tg_duration = int(math.ceil((int(line[1]) - int(line[0])) / 100))
        if tg_duration <= 0:
            tg_duration = 1
        elif tg_duration > 100:
            tg_duration = 100

        if job_id not in id_list:
            if len(id_list) >= JOB_NUM:
                break

            id_list.append(job_id)
            job_index += 1
            data[job_index] = {}
            arrivals[job_index] = int(line[0])
            tg_index = 0

        data[job_index][tg_index] = (num_tasks, tg_duration)
        tg_index += 1

    # aver_num_tg = 0
    # for v in data.values():
    #     aver_num_tg += len(v)
    # print(aver_num_tg/JOB_NUM)

    global NUM_TASKS
    NUM_TASKS = 0
    for tg_info in data.values():
        for v in tg_info.values():
            NUM_TASKS += v[0]

    # 2. Class instance generation
    # Scale the arrival intervals with utilization factor
    # arrivals = np.arange(MAX_ARRIVAL_TU)
    # np.random.shuffle(arrivals)
    min_arrival = min(a for a in arrivals.values())
    for job_index in arrivals.keys():
        arrivals[job_index] -= min_arrival
        arrivals[job_index] = int(np.ceil(SCALE_INTER_ARRIVALS * arrivals[job_index] / UTILIZATION_FACTOR))

    # Collect the number of task groups for each job
    num_task_groups = [len(v) for v in data.values()]

    # Generate sites
    # If a site is not available to some job, the corresponding \mu should be zero. We just skip it here.
    # In real scenarios, you should set \mu based on the resource request and equipment.

    #åŸå§‹ï¼šå‡åŒ€åˆ†å¸ƒ
    capacities = np.random.randint(MIN_MU, MAX_MU + 1, size=(SITE_NUM, JOB_NUM))
    print("å‡åŒ€åˆ†å¸ƒ")

    #å¹‚å¾‹åˆ†å¸ƒ
    # Generate sites capacities with Power Law distribution
    # capacities = np.zeros((SITE_NUM, JOB_NUM), dtype=int)
    # alpha = 1.5  # å¹‚å¾‹æŒ‡æ•°ï¼Œå€¼è¶Šå¤§åˆ†å¸ƒè¶Šé™¡å³­
    # print("alpha=1.5")
    # for site_idx in range(SITE_NUM):
    #     for job_idx in range(JOB_NUM):
    #         # ç”Ÿæˆå¹‚å¾‹åˆ†å¸ƒçš„éšæœºå€¼
    #         power_value = (np.random.pareto(alpha) + 1) * MIN_MU
    #         # é™åˆ¶åœ¨MIN-MAXèŒƒå›´å†…
    #         capacities[site_idx][job_idx] = int(np.clip(power_value, MIN_MU, MAX_MU))

    #å¯¹æ•°æ­£æ€åˆ†å¸ƒ
    # # Generate sites capacities with Log-Normal distribution
    # print("å¯¹æ•°æ­£æ€")
    # mu, sigma = 0.8, 0.6  # åˆ†å¸ƒå‚æ•°ï¼Œéœ€æ ¹æ®å®é™…è°ƒæ•´
    # log_normal_values = np.random.lognormal(mu, sigma, size=(SITE_NUM, JOB_NUM))
    # # ç¼©æ”¾å¹¶è½¬æ¢ä¸ºæ•´æ•°
    # capacities = (log_normal_values - np.min(log_normal_values))
    # capacities = capacities / np.max(capacities) * (MAX_MU - MIN_MU) + MIN_MU
    # capacities = capacities.astype(int)

    #åŠ æƒåˆ†å¸ƒ
    # Define server tiers and weights
    # print("åŠ æƒåˆ†å¸ƒ")
    # TIER_WEIGHTS = {
    #     "high_perf": 9,    # é«˜æ€§èƒ½æœåŠ¡å™¨ (å 10%)
    #     "standard": 5,      # æ ‡å‡†æœåŠ¡å™¨ (å 40%)
    #     "low_power": 2      # ä½åŠŸè€—æœåŠ¡å™¨ (å 50%)
    # }
    # capacities = np.zeros((SITE_NUM, JOB_NUM), dtype=int)
    # tier_types = np.random.choice(
    #     list(TIER_WEIGHTS.keys()), 
    #     size=SITE_NUM,
    #     p=[0.1, 0.4, 0.5]  # å„ç­‰çº§å æ¯”
    # )

    # for site_idx, tier in enumerate(tier_types):
    #     base_capacity = np.random.randint(MIN_MU, int(MAX_MU * 0.8))
    #     weight_factor = TIER_WEIGHTS[tier] / 5.0  # æ ‡å‡†åŒ–æƒé‡å› å­
    
    #     # ä¿®æ­£ï¼šä½¿ç”¨å†…ç½®int()æ›¿ä»£astype()
    #     tier_value = int(base_capacity * weight_factor)  # ğŸŸ¢ å…³é”®ä¿®æ­£ç‚¹
    
    #     capacities[site_idx] = np.clip(
    #         tier_value,  # ä½¿ç”¨è½¬æ¢åçš„æ•´æ•°å€¼
    #         MIN_MU, 
    #         MAX_MU
    #     )

    #æ··åˆåˆ†å¸ƒ
    # Hybrid distribution (Power Law + Log-Normal)
    # capacities = np.zeros((SITE_NUM, JOB_NUM), dtype=int)
    # alpha = 1.2  # å¹‚å¾‹æŒ‡æ•°
    # mu, sigma = 0.7, 0.4  # å¯¹æ•°æ­£æ€å‚æ•°
    # for site_idx in range(SITE_NUM):
    #     # åŸºç¡€å€¼é‡‡ç”¨å¹‚å¾‹åˆ†å¸ƒ
    #     base_value = (np.random.pareto(alpha) + 1) * MIN_MU
    
    #     for job_idx in range(JOB_NUM):
    #         # å åŠ å¯¹æ•°æ­£æ€æ³¢åŠ¨
    #         fluctuation = np.random.lognormal(mu, sigma)
    #         value = base_value * fluctuation
        
    #         capacities[site_idx][job_idx] = int(np.clip(value, MIN_MU, MAX_MU))

    for site_idx in range(SITE_NUM):
        site = Site(site_idx, capacities=capacities[site_idx])
        sites.append(site)

    # Generate available sites with Zipf distribution
    shuffled_sites = np.arange(SITE_NUM)
    np.random.shuffle(shuffled_sites)

    #å¥‡æ™®å¤«åˆ†å¸ƒ
    probs = np.power([1 / (i + 1) for i in shuffled_sites], ALPHA)
    if ALPHA == 0:
        probs = probs / SITE_NUM
    else:
        probs = probs / sum(probs)

    #å¯¹æ•°æ­£æ€åˆ†å¸ƒ
    # mu, sigma = 0, 0.5  # å‚æ•°éœ€æ ¹æ®æ•°æ®è°ƒæ•´
    # log_probs = np.random.lognormal(mu, sigma, size=len(shuffled_sites))
    # probs = log_probs / sum(log_probs)
    # print("å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼šmu, sigma = 0, 0.5")

    #æŒ‡æ•°åˆ†å¸ƒ
    # scale = 1.0  # æ§åˆ¶è¡°å‡é€Ÿç‡
    # exp_probs = np.random.exponential(scale, size=len(shuffled_sites))
    # probs = exp_probs / sum(exp_probs)
    # print("æŒ‡æ•°åˆ†å¸ƒ")

    #å‡åŒ€åˆ†å¸ƒ
    # probs = np.ones(len(shuffled_sites)) / len(shuffled_sites)
    # print("å‡åŒ€åˆ†å¸ƒ")

    #æ··åˆåˆ†å¸ƒï¼šå¹‚å¾‹+å¥‡æ™®å¤«
    # zipf_probs = np.power([1/(i+1) for i in shuffled_sites], ALPHA)
    # uniform_probs = np.ones(len(shuffled_sites))
    # probs = 0.7 * (zipf_probs/sum(zipf_probs)) + 0.3 * (uniform_probs/len(shuffled_sites))  # æƒé‡å¯è°ƒ
    # print("æ··åˆåˆ†å¸ƒ")

    for job_idx in range(JOB_NUM):
        job = Job(job_idx, task_groups=None, arrival_time=arrivals[job_idx])

        # Generate the number of tasks for each task group
        num_tasks = []
        task_durations = []
        for tg_info in data[job_idx].values():
            num_tasks.append(tg_info[0])
            task_durations.append(tg_info[1])

        # Generate AS set for this task group
        num_available_sites = np.random.randint(MIN_AS_NUM_FOR_TG, MAX_AS_NUM_FOR_TG + 1, size=num_task_groups[job_idx])

        tgs = []
        for tg_idx in range(num_task_groups[job_idx]):
            start_site_idx = np.random.choice(list(range(SITE_NUM)), p=probs)
            as_set = shuffled_sites[start_site_idx: start_site_idx + num_available_sites[tg_idx] - 1]
            available_sites = {site for site in sites if site.index in as_set}

            tg = TaskGroup(tg_idx, job, num_tasks=num_tasks[tg_idx], available_sites=available_sites)
            tg.real_running_time = task_durations[tg_idx]

            tgs.append(tg)

        job.task_groups = tgs
        job.set_available_sites()

        jobs.append(job)


def print_env():
    print("----------------------- Job Info -----------------------")
    for job in jobs:
        print(job)
    print("\n----------------------- Site Info -----------------------")
    for site in sites:
        print(site)
    print("\n----------------------- Summary -----------------------")
    print("There are {} tasks, {} jobs, {} sites, {} util, {} alpha".format(NUM_TASKS, JOB_NUM, SITE_NUM, UTILIZATION_FACTOR, ALPHA))
    print("MAX_AS_NUM_FOR_TG: {}, MIN_AS_NUM_FOR_TG: {}, MIN_MU: {}, MAX_MU: {}".format(MAX_AS_NUM_FOR_TG, MIN_AS_NUM_FOR_TG, MIN_MU, MAX_MU))


def get_Kg(job: Job):
    """
    Return a map from each site's index to the list of its processable task groups' index.
    """
    return {site.index: [tg.index for tg in job.task_groups if site in tg.available_sites]
            for site in job.available_sites}


def check_validity():
    """
    Check the validity of the generated env.
    """
    for job in jobs:
        assert 0 <= job.index < JOB_NUM
        assert 0 <= job.arrival_time < MAX_ARRIVAL_TU
        assert job.completion_time is None

        for tg in job.task_groups:
            assert 0 <= tg.index < MAX_TG_NUM
            assert tg.job is job
            assert MIN_TASK_NUM_IN_TG <= tg.num_tasks <= MAX_TASK_NUM_IN_TG
            assert tg.num_unfinished_tasks == tg.num_tasks

            if MODE == HOMOGENEOUS_MODE:
                assert tg.real_running_time == 1
            else:
                assert tg.real_running_time >= 1

            assert not (False in [site in job.available_sites for site in tg.available_sites])

        assert not (False in [site in sites for site in job.available_sites])

        Kg = get_Kg(job)
        assert not (False in [0 <= site_idx < SITE_NUM for site_idx in Kg.keys()])
        for site_idx, tg_idx_list in Kg.items():
            for tg_idx in tg_idx_list:
                assert 0 <= tg_idx < len(job.task_groups)
                assert sites[site_idx] in job.task_groups[tg_idx].available_sites

    for site in sites:
        assert 0 <= site.index < SITE_NUM
        assert not (False in [MIN_MU <= cap <= MAX_MU for cap in site.capacities])

    print("Assertion pass!")


def reset():
    """
    Reset the scheduling result to empty.
    """
    for job in jobs:
        job.reset()
        for tg in job.task_groups:
            tg.reset()
    for site in sites:
        site.reset()


# 1. Synthesizing
# create_env()
# check_validity()
# print_env()

# 2. From trace
from_trace()
print_env()
